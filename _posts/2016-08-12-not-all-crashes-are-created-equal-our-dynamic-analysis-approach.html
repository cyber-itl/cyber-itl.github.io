---
layout: post
title: 'Not All Crashes are Created Equal: Our Dynamic Analysis Approach'
categories: []
tags: []
status: draft
type: post
published: false
meta: {}
excerpt_separator: <!--more-->
---
<p>We fuzz software using a modified version of <a href="http://lcamtuf.coredump.cx/afl/">American Fuzzy Lop</a>&nbsp;(AFL), every cutting edge attacker's current <a href="https://twitter.com/thedavidbrumley/status/762107471771021313">fuzzer of choice</a>.</p><p>When people fuzz software, they generally sort the resulting crashes into two categories: "real" results, meaning ones that are likely to be exploitable, and "false positive" results, which they discard as being not useful to their purposes. &nbsp;Really, though, all crashes are potentially relevant to a software consumer's purchasing decision. &nbsp;Different organizations have different risk profiles, and a crash type that might not matter to one organization might be a deal breaker for another. &nbsp;</p><p>We sort fuzzing and crash testing results into three categories:&nbsp;</p><ul><li>Exploitable</li><li>Disruptable</li><li>Algorithmic Complexity</li></ul><p>Some organizations care the most about exploitability. &nbsp;A bank, for example, needs to be able to trust their data, and they'd rather that a system fail than that it be propagating bad information. &nbsp;On the other hand, an organization with an offshore oil rig would prefer that their system be exploited and being used for warez distribution than that it actually crash and shut down operations. &nbsp;If their system crashes and the drill stops drilling, that could take them a really long time to recover from. &nbsp;So, disruptability is their highest priority. &nbsp;Finally, algorithmic complexity isn't on everyone's radar yet, but it's going to be a big deal in a few years. &nbsp;Large organizations like Google and Twitter protect themselves from DDOS by making sure that their operations are highly decentralized and parallelized. &nbsp;What they can't protect against, though, are algorithmic complexity attacks. &nbsp;If they have a particular worst case input that turns their hash table into a linked list, or some similar algorithmic vulnerability, then a very low-resource attack could bring their operations to a crawl. &nbsp;We have a modified version of AFL that we use to find cases where the same sized inputs can create longer and longer runtimes, allowing us to assess the software's vulnerability to this type of attack.</p><p>Our static analysis is very fast and automated, whereas this dynamic analysis process takes a lot more time and resources. &nbsp;But, this constitutes the "ground truth" when it comes to software risk and vulnerability. &nbsp;The types and numbers of crashes we get also indicate how much testing of this nature occurred behind the scenes - if the vendor tested thoroughly pre-release, we'll see a lot fewer results than if they didn't. &nbsp;</p><p>We're taking a page from the EPA's book and using predictive mathematical models to cut down on the amount of work we need to do. &nbsp;We test a statistically significant number of binaries, and then we use Bayesian analysis and linear regression to predict the other results based on the statistic analysis features. &nbsp;</p>
