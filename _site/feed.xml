<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.3">Jekyll</generator><link href="https://cyber-itl.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://cyber-itl.org/" rel="alternate" type="text/html" /><updated>2018-12-14T08:44:14-08:00</updated><id>https://cyber-itl.org/</id><title type="html">Cyber Independent Testing Lab</title><subtitle>The Cyber Independent Testing Lab (CITL) works for a fair, just, and safe software marketplace for all consumers, empowering consumers to protect themselves.</subtitle><entry><title type="html">A look at home routers, and a surprising bug in Linux/MIPS</title><link href="https://cyber-itl.org/2018/12/07/a-look-at-home-routers-and-linux-mips.html" rel="alternate" type="text/html" title="A look at home routers, and a surprising bug in Linux/MIPS" /><published>2018-12-07T00:00:00-08:00</published><updated>2018-12-07T00:00:00-08:00</updated><id>https://cyber-itl.org/2018/12/07/a-look-at-home-routers-and-linux-mips</id><content type="html" xml:base="https://cyber-itl.org/2018/12/07/a-look-at-home-routers-and-linux-mips.html">&lt;p&gt;We reviewed 28 popular home routers for basic hardening features. None performed well. Oh, and we found a bug in the Linux/MIPS architecture.&lt;!--more--&gt;&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;Today we're pleased to announce the release of two papers:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;i&gt;&lt;a href=&quot;/assets/papers/2018/build_safety_of_software_in_28_popular_home_routers.pdf&quot;&gt;Build Safety of Software in 28 Popular Home Routers&lt;/a&gt;&lt;/i&gt;, by Parker Thompson and Sarah Zatko&lt;/li&gt;
&lt;li&gt;&lt;i&gt;&lt;a href=&quot;/assets/papers/2018/Linux_MIPS_missing_foundations.pdf&quot;&gt;Linux MIPS - A soft target: past, present, and future&lt;/a&gt;&lt;/i&gt;, by Parker Thompson and Mudge Zatko&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
In the first paper, we analyze the firmware images of 28 popular home routers, checking for basic code hygiene and software safety features. What we found was disappointing: none of the routers made consistent use of basic &lt;a href=&quot;/about/methodology/#safety-features&quot;&gt;software safety features&lt;/a&gt; like &lt;a href=&quot;/about/glossary/#a&quot;&gt;ASLR&lt;/a&gt;, &lt;a href=&quot;/about/glossary/#s&quot;&gt;stack guards&lt;/a&gt;, and &lt;a href=&quot;/about/glossary/#d&quot;&gt;DEP&lt;/a&gt; - features which have been standard in desktop environments for over 15 years.
&lt;/p&gt;

&lt;p&gt;
Given the role these devices play in consumers' homes, and the ease with which these issues could be resolved, we believe the absence of these features is reckless and negligent. We strongly urge vendors to review their software build practices and adopt practices which ensure these basic security features are present prior to product release.
&lt;/p&gt;

&lt;p&gt;
But that's not all. In the second paper, we describe an unfortunate bug in the Linux/MIPS architecture which we encountered in the course of our reporting on routers. This bug, whose origins date back to 2001, prevents most Linux/MIPS binaries from enjoying the full protections of DEP and ASLR. Given the popularity of Linux/MIPS in embedded devices (such as IoT, consumer and enterprise network equipment, etc), and the enormous diversity of threat models for such devices, we believe this bug represents a significant risk to a large segment of Internet-connected devices.
&lt;/p&gt;</content><author><name></name></author><summary type="html">We reviewed 28 popular home routers for basic hardening features. None performed well. Oh, and we found a bug in the Linux/MIPS architecture.</summary></entry><entry><title type="html">Building on research success, CITL grows and focuses on scale</title><link href="https://cyber-itl.org/2017/05/01/building-on-research-success-citl-grows-and-focuses-on-scale.html" rel="alternate" type="text/html" title="Building on research success, CITL grows and focuses on scale" /><published>2017-05-01T00:00:00-07:00</published><updated>2017-05-01T00:00:00-07:00</updated><id>https://cyber-itl.org/2017/05/01/building-on-research-success-citl-grows-and-focuses-on-scale</id><content type="html" xml:base="https://cyber-itl.org/2017/05/01/building-on-research-success-citl-grows-and-focuses-on-scale.html">&lt;p&gt;In the past year a lot has happened at CITL&lt;!--more--&gt;:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;We compared the utilization of application armoring techniques across thousands of applications within Windows 10, Linux Ubuntu and Linux Gentoo, and Mac OS X.&lt;/li&gt;&lt;!--more--&gt;&lt;li&gt;We hacked up a proof-of-concept of our toolchain and looked under the hood at IoT applications and operating systems in use on “Smart” TVs.&amp;nbsp;&lt;/li&gt;&lt;li&gt;We found safety issues that seemed to have gone unnoticed for years in popular applications (e.g. Firefox and Office 2011 on OS X).&amp;nbsp;&lt;/li&gt;&lt;li&gt;We provided both high level and detailed results of our findings and approaches in our presentations at BlackHat and Defcon.&amp;nbsp;&lt;/li&gt;&lt;li&gt;We collaborated with Consumer Reports and other public interest organizations to codify how to assess security risks to consumers in the products they buy.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;We're proud of these achievements, but we're hungry for more. Today we're happy to make several announcements on that front.&lt;/p&gt;&lt;p&gt;First, having established the success of CITL's analytic methodology, Director Peiter &quot;Mudge&quot; Zatko will transition to a board position, where he will continue to provide vision and strategic insight.&lt;/p&gt;&lt;p&gt;Now that we have a proof of concept in hand it is time for us to grow and focus on scale, operations, and getting more information out to the public and industry.&amp;nbsp;&lt;/p&gt;&lt;p&gt;To help CITL expand its methodology to larger studies, we are pleased to welcome Tim Carstens as Acting Director. Though a mathematician by training, Carstens joins CITL with over a decade of experience in computer security, having reviewed systems in use by hundreds of millions of people and businesses. &amp;nbsp;&lt;/p&gt;&lt;p&gt;Carstens will assist Chief Scientist Sarah Zatko as CITL continues to expand our methodology to larger studies at greater scale.&lt;/p&gt;&lt;p&gt;Speaking of larger studies, we are very excited about our third announcement: in light of our success applying the CITL methodology to desktop applications, DARPA has provided further funding toexpand our methodology to software running in embedded devices and IoT.&lt;/p&gt;&lt;p&gt;The ubiquity of such devices, together with the difficulty of detecting when one has been hacked, makes them ideal targets for attackers. Even those devices which seem completely mundane can still be useful as a node in a botnet. Thus, the poor security of such devices effects us all. As always, we look forward to sharing with you our progress as we study and compare the software that runs on these devices.&lt;/p&gt;&lt;p&gt;Fourth, we are thankful to announce that we've recently received a large charitable donation from a company in the financial sector. Their donation in support of the CITL mission will allow us to expand our team and perform larger, more in-depth reports on more classes of software. We are truly appreciative of their vote of confidence.&lt;/p&gt;&lt;p&gt;We look forward to continuing our relationship with our partners, including Consumer Reports and the Ford Foundation, and to new partnerships to come.&lt;/p&gt;</content><author><name></name></author><summary type="html">In the past year a lot has happened at CITL</summary></entry><entry><title type="html">CITL Status Report</title><link href="https://cyber-itl.org/our_approach/2017/01/30/citl-status-report.html" rel="alternate" type="text/html" title="CITL Status Report" /><published>2017-01-30T00:00:00-08:00</published><updated>2017-01-30T00:00:00-08:00</updated><id>https://cyber-itl.org/our_approach/2017/01/30/citl-status-report</id><content type="html" xml:base="https://cyber-itl.org/our_approach/2017/01/30/citl-status-report.html">&lt;p&gt;Some people have been asking when they're going to get to see all the great output and data we're generating, so this seemed like a good time to explain where we're at right now.&lt;!--more--&gt; &amp;nbsp;We've realized that while we're busy executing on this plan, maybe other people would like to know more about the behind the scenes also. &amp;nbsp;In order to have automatically generated reports and software scores available at scale, here's what needs to happen:&amp;nbsp;&lt;/p&gt;&lt;p&gt;1. Automate Static Analysis measurement collection - This part is done for Windows, OSX, and Linux/ELF environments (Intel and ARM). &amp;nbsp;It takes ~1 second per binary and we're confident in the accuracy of the results we're collecting. &amp;nbsp;&lt;/p&gt;&lt;p&gt;2. Collect a lot of fuzzing and crash test data - Well underway, but still ongoing. &amp;nbsp;We've got about 100 cores chugging away, and enough results now to be moving on to the next step.&lt;/p&gt;
  
      &lt;img src=&quot;/assets/images/blog/citl-status-report-1.jpg&quot; alt=&quot;&quot;/&gt;
  

&lt;p&gt;3. Correlate dynamic analysis results (2) with static analysis results (1) to finalize score calculations. &amp;nbsp;This is what we're working on now, and it's the main thing that has to happen before we're happy with releasing reports at scale. &amp;nbsp;&lt;/p&gt;&lt;p&gt;4. (Reach Goal) Gain enough confidence in our mathematical model to successfully predict dynamic results based on static results. &amp;nbsp;This will allow us to present estimated crash test results based on easily automated analysis. &amp;nbsp;&lt;/p&gt;&lt;p&gt;Why is this third step so important? &amp;nbsp;While we know that some things make software safer (ASLR, stack guards, DEP, source fortification, etc) and that some things make software weaker (using historically unsafe functions, high complexity, etc), the industry needs better data on how much they impact software safety. &amp;nbsp;If a perfect score is 100, how many points is having ASLR worth? &amp;nbsp;Linking to insecure libraries certainly introduces risk, but how much should it impact the overall score? &amp;nbsp;We want to have a better answer to questions like these before we publish our first official software safety reports. &amp;nbsp;Having a strong model to support our risk assessments will provide our ratings with the credibility they need in order to influence consumers, developers, the security community, and the commercial world. &amp;nbsp;&lt;/p&gt;&lt;p&gt;In the meantime, here's an &lt;a href=&quot;/2016/09/01/our-static-analysis-metrics-and-features.html&quot;&gt;overview&lt;/a&gt;&amp;nbsp;of the sorts of software properties we're measuring with our static analysis. &amp;nbsp;Also, stay tuned! &amp;nbsp;We're hoping to have some exciting new partnerships and efforts to announce in the coming months. &amp;nbsp;&lt;/p&gt;</content><author><name></name></author><summary type="html">Some people have been asking when they're going to get to see all the great output and data we're generating, so this seemed like a good time to explain where we're at right now.</summary></entry><entry><title type="html">To Upgrade or not? A look at Office 2016 for OSX</title><link href="https://cyber-itl.org/2017/01/04/to-upgrade-or-not-a-look-at-office-2016-for-osx.html" rel="alternate" type="text/html" title="To Upgrade or not?  A look at Office 2016 for OSX" /><published>2017-01-04T00:00:00-08:00</published><updated>2017-01-04T00:00:00-08:00</updated><id>https://cyber-itl.org/2017/01/04/to-upgrade-or-not-a-look-at-office-2016-for-osx</id><content type="html" xml:base="https://cyber-itl.org/2017/01/04/to-upgrade-or-not-a-look-at-office-2016-for-osx.html">&lt;p&gt;When a new suite of a familiar piece of software comes out, you have to decide if you want to upgrade or not.&lt;!--more--&gt; &amp;nbsp;Sometimes there's a long-awaited feature, and you can't wait to get the latest and greatest, and sometimes it looks an awful lot like what you've already got, so it doesn't really seem worth the bother. &amp;nbsp;The security and software risk profiles of the old vs new versions are another factor that should be part of this consideration. &amp;nbsp;&lt;/p&gt;&lt;p&gt;When we looked at &lt;a href=&quot;/data/our_approach/2016/09/15/a-closer-look-at-the-osx-continuum.html&quot;&gt;scores for OSX applications&lt;/a&gt;, the Microsoft Office 2011 suite was at the bottom of its category, and the accompanying Microsoft AutoUpdate application was at the bottom of the whole OSX environment. &amp;nbsp;Since then we've been asked how Office 2016 stacks up in comparison, and it provides an excellent example of hidden benefits of an upgrade, as it has a much better risk profile than the 2011 suite. &amp;nbsp;&lt;/p&gt;&lt;p style=&quot;margin-left:0in; margin-right:0in&quot;&gt;Where the 2011 suite was mostly 32 bit files, largely without ASLR, the 2016 had all 64 bit files, all with ASLR.&amp;nbsp; Stack guards are now consistently employed, and function fortification is used in 58% of the files, as compared to 5% before.&amp;nbsp; The main applications (e.g. Word.app, Excel.app, etc.) were at the bottom of the pack in the 2016 suite, meaning that these had not improved by much, but the overall score distributions, which take into account all of the binary applications and the libraries against which they are linked,&amp;nbsp;improved greatly.&amp;nbsp;&amp;nbsp;&lt;/p&gt;
  
       &lt;figure&gt;&lt;img src=&quot;/assets/images/blog/to-upgrade-or-not-a-look-at-office-2016-for-osx-1.png&quot; alt=&quot; The histogram shows the score distributions for a base install of OSX (El Capitan), with the 5th, 50th, and 95th percentile scores called out. &amp;nbsp;Each triangle corresponds to one of the scores called out on the hardening line below. &amp;nbsp;We're comparing the Average score, the score for the AutoUpdate binary, and the scores for the main applications (Word, Excel, and PowerPoint) between Office 2011 and Office 2016. &amp;nbsp; &quot;/&gt;&lt;figcaption&gt;The histogram shows the score distributions for a base install of OSX (El Capitan), with the 5th, 50th, and 95th percentile scores called out. &amp;nbsp;Each triangle corresponds to one of the scores called out on the hardening line below. &amp;nbsp;We're comparing the Average score, the score for the AutoUpdate binary, and the scores for the main applications (Word, Excel, and PowerPoint) between Office 2011 and Office 2016.&lt;/figcaption&gt;&lt;/figure&gt;
  

&lt;table data-preserve-html-node=&quot;true&quot;&gt;
    &lt;thead&gt;&lt;tr data-preserve-html-node=&quot;true&quot;&gt;
    &lt;th data-preserve-html-node=&quot;true&quot;&gt;&lt;/th&gt;
    &lt;th data-preserve-html-node=&quot;true&quot; width=&quot;40p&quot; align=&quot;right&quot;&gt;2011&lt;/th&gt;
    &lt;th data-preserve-html-node=&quot;true&quot; width=&quot;40p&quot; align=&quot;right&quot;&gt;2016&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;
  &lt;tbody&gt;
  &lt;tr data-preserve-html-node=&quot;true&quot;&gt;
    &lt;td data-preserve-html-node=&quot;true&quot;&gt;Average Score&lt;/td&gt;
    &lt;td data-preserve-html-node=&quot;true&quot; align=&quot;right&quot;&gt;16.5&lt;/td&gt;
    &lt;td data-preserve-html-node=&quot;true&quot; align=&quot;right&quot;&gt;77&lt;/td&gt;
  &lt;/tr&gt;
   &lt;tr data-preserve-html-node=&quot;true&quot;&gt;
    &lt;td data-preserve-html-node=&quot;true&quot;&gt;Minimum Score&lt;/td&gt;
    &lt;td data-preserve-html-node=&quot;true&quot; align=&quot;right&quot;&gt;-5&lt;/td&gt;
    &lt;td data-preserve-html-node=&quot;true&quot; align=&quot;right&quot;&gt;54&lt;/td&gt;
  &lt;/tr&gt;
   &lt;tr data-preserve-html-node=&quot;true&quot;&gt;
    &lt;td data-preserve-html-node=&quot;true&quot;&gt;Maximum Score&lt;/td&gt;
    &lt;td data-preserve-html-node=&quot;true&quot; align=&quot;right&quot;&gt;80&lt;/td&gt;
    &lt;td data-preserve-html-node=&quot;true&quot; align=&quot;right&quot;&gt;92.5&lt;/td&gt;
  &lt;/tr&gt;

&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;The Microsoft AutoUpdate file that was such an egregious offender also improved significantly, with a new score of 64.&amp;nbsp; It still has risky and ick functions (two calls to system(), which should make anyone in the security field a bit nervous about general hygiene here...), but now has stack guards, ASLR, partial fortification, good functions, and is 64 bit.&amp;nbsp;&lt;/p&gt;&lt;p&gt;The story wasn’t 100% positive, however – the Microsoft Error Reporting (MERP) application had a call to an ick function, system(),&amp;nbsp;that hadn’t been present in the 2011 MERP files.&amp;nbsp; These sorts of bad and ick functions weren’t seen in Microsoft products for Windows, which implies that they might still be less strict about developer practices for OSX products.&amp;nbsp; Both suites had risky functions in most their files.&amp;nbsp; 2016 had slightly more, but this could be because our risky function list was expanded to be more complete in its ANSI v POSIX coverage between the two data collections. [As a reminder:&amp;nbsp;when we reveal our dynamic runtime analysis scores later this year we hope to share more information on our function lists and the math behind their weightings!]&lt;/p&gt;&lt;p&gt;Overall, the 2016 security stance was much improved, with much more consistent use of application armoring features, and much better overall static feature scores. &amp;nbsp;Perhaps most importantly, the AutoUpdate liability was significantly decreased. &amp;nbsp;This isn't a security feature that would be heavily advertised by the vendor, but it might be attractive to someone making the decision on whether to upgrade their corporate environment or not. &amp;nbsp;&lt;/p&gt;&lt;p&gt;Looking at changes in software packages in this way helps quantify and prioritize which upgrades may significantly decrease your security exposure, and which ones are more simply designed as feature enhancements aimed at selling you incremental revisions of the same tool. In the case of Office for OSX, removing Office 2011 and replacing it with Office 2016 is a measurable reduction in risk to your environment. However, we still do not see sandboxing, strict hygiene regarding bad and risky functions, and other advanced safety features in their OS X products (e.g. CFI and ShadowStacks) they way we do in their Windows 10 offerings.&lt;/p&gt;</content><author><name></name></author><summary type="html">When a new suite of a familiar piece of software comes out, you have to decide if you want to upgrade or not.</summary></entry><entry><title type="html">Fortify Source: A Deeper Dive into Function Hardening on Linux and OS X</title><link href="https://cyber-itl.org/data/2016/11/21/fortify-source-a-deeper-dive-into-the-data.html" rel="alternate" type="text/html" title="Fortify Source: A Deeper Dive into Function Hardening on Linux and OS X" /><published>2016-11-21T00:00:00-08:00</published><updated>2016-11-21T00:00:00-08:00</updated><id>https://cyber-itl.org/data/2016/11/21/fortify-source-a-deeper-dive-into-the-data</id><content type="html" xml:base="https://cyber-itl.org/data/2016/11/21/fortify-source-a-deeper-dive-into-the-data.html">&lt;p&gt;Source fortification is a powerful tool in modern compilers. &amp;nbsp;When enabled, the compiler will inspect the code and attempt to automatically replace risky functions&lt;!--more--&gt; with safer, better-bounded versions. &amp;nbsp;Of course, the compiler can only do that if it can figure out what those bounds should be, which isn't always easy. &amp;nbsp;The developer does not get much feedback as to the success rate of this process, though. &amp;nbsp;The developer knows that they may have enabled source code fortification (-D_FORTIFY_SOURCE), but they do not get a readout on how many of their memcpy instances are now replaced with the safer memcpy_chk function, for example. This is important to the consumer because just looking to see that a good software build practice was intended does not reveal whether the practice actually improved the safety in the resulting application. That made us really curious to dig into the data on source fortification and its efficacy.&amp;nbsp;&lt;/p&gt;&lt;p&gt;We first looked at the fortification statistics on Linux. &amp;nbsp;The following numbers only deal with the 25% of Linux binaries (2631 files) where fortification was enabled. &amp;nbsp;Linux compilers, at the time we performed our initial analysis,&amp;nbsp;have 72 functions that they inspect for potential fortification, and the binaries we inspected had a total of almost 2 million instances of those 72 functions.&amp;nbsp;&amp;nbsp;Of those 2 million instances with functions that could potentially made safer, 91.7% were fortified! &amp;nbsp;Well done, Linux. Of course, those 165,000+ functions that remain in their less secure form warrant some concern and the there's still further good and bad news when you look at all of this more closely. &amp;nbsp;We viewed this data two different ways. &amp;nbsp;First we looked at it broken out by binary, then by function. &amp;nbsp;&lt;/p&gt;&lt;p&gt;This chart shows the percent fortification for all binary files on our default base install of Ubuntu Linux. &amp;nbsp;31% of files were 95-100% fortified. &amp;nbsp;Many of these had large function counts, with some as high as 25k fortified function instances. &amp;nbsp;The rest of the files were evenly distributed from 0 to 95%. &amp;nbsp;While these binaries generally had lower function counts, there were exceptions to this rule. &amp;nbsp;Most notable was /lib/systemd/systemd, which had over 42k unfortified pread instances.&amp;nbsp;&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;/assets/images/blog/fortify-source-a-deeper-dive-into-the-data-1.png&quot; alt=&quot; Fortification by binary on Ubuntu Linux. &amp;nbsp;Each dot is a binary. &amp;nbsp;The x axis shows what percent of its relevant functions were fortified, and the y axis shows how many fortifiable functions the binary had overall. &amp;nbsp;This chart excludes /lib/systemd/systemd for readability, as it had 43,212 functions, but was 0.7% fortified. &quot; /&gt;  &lt;figcaption&gt;Fortification by binary on Ubuntu Linux. &amp;nbsp;Each dot is a binary. &amp;nbsp;The x axis shows what percent of its relevant functions were fortified, and the y axis shows how many fortifiable functions the binary had overall. &amp;nbsp;This chart excludes /lib/systemd/systemd for readability, as it had 43,212 functions, but was 0.7% fortified.&lt;/figcaption&gt;&lt;/figure&gt;


&lt;p&gt;&amp;nbsp;When we view the fortification data by function, as shown in the graphic below, we see that most functions are at one extreme or the other. &amp;nbsp;15 are 95-100% fortified, while 39 were 0-5% fortified. &amp;nbsp;Of those 39 functions, 28 of them were *never* fortified. &amp;nbsp;Many of these totally unfortified functions are less common, with an average of 302 total instances across the 2,631 Ubuntu files we examined. &amp;nbsp;On the other hand, the highly fortified functions had an average of 104k total instances. &amp;nbsp;The most common function overall was sprintf, which had 1.3 million instances and was over 99.9% fortified. &amp;nbsp;&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/assets/images/blog/fortify-source-a-deeper-dive-into-the-data-2.png&quot; alt=&quot; Fortification by function in Ubuntu Linux. &amp;nbsp;The x axis shows percent fortification broken up into bins that are 5 points wide. &amp;nbsp;The y axis shows how many functions fall in each bin. &amp;nbsp; &quot; /&gt;  &lt;figcaption&gt;Fortification by function in Ubuntu Linux. &amp;nbsp;The x axis shows percent fortification broken up into bins that are 5 points wide. &amp;nbsp;The y axis shows how many functions fall in each bin. &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;p&gt;So, function fortification is a fairly mature feature on Linux, although imperfect. &amp;nbsp;It does well on many commonly used risky functions, and fortifies 91% of files overall. &amp;nbsp;Still, over 2/3 of the binaries that are intended to be fortified end up being less than 95% fortified, and the developer gets no feedback as to the success rate for their particular case.&amp;nbsp;&amp;nbsp;&lt;/p&gt;&lt;p&gt;On OSX, however, we see that this security feature is much less mature. &amp;nbsp;For OSX we had more software installed, so we had almost 7 million function instances observed. &amp;nbsp;21.4% of these were fortified, which is fairly dismal, especially when compared to Linux's 91.7%. &amp;nbsp;&lt;/p&gt;&lt;p&gt;In fact, while Linux had many binaries with high function counts that were almost entirely fortified, no OSX binaries with high function counts got high fortification scores. &amp;nbsp;The most fortified functions in a 100% fortified file was 121. &amp;nbsp;&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;/assets/images/blog/fortify-source-a-deeper-dive-into-the-data-3.png&quot; alt=&quot; Fortification by binary on OSX (El Capitan).&amp;nbsp;&amp;nbsp;Each dot is a binary. &amp;nbsp;The x axis shows what percent of its relevant functions were fortified, and the y axis shows how many fortifiable functions the binary had overall. &amp;nbsp; &quot; /&gt;  &lt;figcaption&gt;Fortification by binary on OSX (El Capitan).&amp;nbsp;&amp;nbsp;Each dot is a binary. &amp;nbsp;The x axis shows what percent of its relevant functions were fortified, and the y axis shows how many fortifiable functions the binary had overall. &lt;/figcaption&gt;&lt;/figure&gt;


&lt;p&gt;The picture is similar when OSX fortification is viewed by function. &amp;nbsp;52 functions are never fortified. &amp;nbsp;The most successful function is strcat, with 93% fortification, followed by strncat and sprintf (83% and 82%, respectively). &amp;nbsp;Interestingly, we found that a couple functions had higher fortification rates in OSX than they did in Linux. &amp;nbsp;&amp;nbsp;&lt;/p&gt;&lt;p&gt;In both Linux and OSX, we observed that string functions like these had higher success rates than functions dealing with (dynamic) memory pointers.&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;/assets/images/blog/fortify-source-a-deeper-dive-into-the-data-4.png&quot; alt=&quot; Histogram of fortification by function on OSX. &amp;nbsp;The x axis shows percentage fortified, with a bin width of 5. &amp;nbsp;The y axis shows how many functions fell into each bin. &amp;nbsp;Nothing was 95-100% fortified. &quot; /&gt;&lt;figcaption&gt;Histogram of fortification by function on OSX. &amp;nbsp;The x axis shows percentage fortified, with a bin width of 5. &amp;nbsp;The y axis shows how many functions fell into each bin. &amp;nbsp;Nothing was 95-100% fortified.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;So, OSX has some catching up to do, compared to their Linux equivalent. This was a relatively unexpected result,&amp;nbsp;but that's why this sort of data analysis is important. &amp;nbsp;&lt;/p&gt;</content><author><name></name></author><category term="fortify-source" /><category term="static-analysis" /><category term="data" /><category term="linux" /><category term="osx" /><summary type="html">Source fortification is a powerful tool in modern compilers. &amp;nbsp;When enabled, the compiler will inspect the code and attempt to automatically replace risky functions</summary></entry><entry><title type="html">Revisiting the Linux Score Distribution</title><link href="https://cyber-itl.org/2016/11/15/revisiting-the-linux-score-distribution.html" rel="alternate" type="text/html" title="Revisiting the Linux Score Distribution" /><published>2016-11-15T00:00:00-08:00</published><updated>2016-11-15T00:00:00-08:00</updated><id>https://cyber-itl.org/2016/11/15/revisiting-the-linux-score-distribution</id><content type="html" xml:base="https://cyber-itl.org/2016/11/15/revisiting-the-linux-score-distribution.html">&lt;p&gt;A while back, we showed what the score distributions were for base installs of three major platforms.&lt;!--more--&gt; &amp;nbsp;Here, we're going to compare that base install view of Linux with the score distribution for a custom, hardened instance of Linux. &amp;nbsp;To refresh memories, here was the original Linux distribution:&amp;nbsp;&lt;/p&gt;
  
      &lt;img src=&quot;/assets/images/blog/revisiting-the-linux-score-distribution-1.png&quot; alt=&quot;&quot;/&gt;
  

&lt;p&gt;And here is the score distribution for our customized, hardened version of Linux:&amp;nbsp;&lt;/p&gt;
  
&lt;img src=&quot;/assets/images/blog/revisiting-the-linux-score-distribution-2.png&quot; alt=&quot;&quot; /&gt;
  

&lt;p&gt;There are no longer any negative scores, and the overall score distribution has greatly improved. &amp;nbsp;These numbers can be compared directly, because they're both from environments with the same set of possible safety features, and all features are weighted the same.&amp;nbsp;&lt;/p&gt;&lt;p&gt;As we saw with Windows, you can tell from the histogram of the hardened Linux instance that the build practices were more uniform because the binaries are much more clustered than before. &amp;nbsp;This customization largely consisted of re-compiling all of the binaries with all possible application armoring features enabled to reasonable degrees. &amp;nbsp;Bad functions or programming errors in the source code were not addressed, because the source code was not changed, but all available safety features are now being applied across the board. &amp;nbsp;It should be noted that this hardened Linux build still lacks certain Kernel safety features &amp;nbsp;(e.g. not all of the PAX/GRSec Kernel mods were enabled)&amp;nbsp;as some would have interfered with necessary operations. &amp;nbsp;This is just a reasonably hardened, but entirely usable Linux instance. &amp;nbsp;&lt;/p&gt;&lt;p&gt;The ability to easily customize and harden the default distribution of Open Source Operating Systems should be a big draw for organizations that want to improve their security stance. &amp;nbsp;With closed source software, the consumer has to hope that the vendor in question knows what they're doing when it comes to build environment and settings (Microsoft Windows 10 did well in this regard when compared with default OS X and Linux builds).&amp;nbsp;The results usually end up being mixed and somewhat mediocre. &amp;nbsp;The off the shelf quality of open source build practices isn't much better, as shown by the distribution for Ubuntu above, but then at least the consumer can redo it on their own and know it was done right. &amp;nbsp;This sort of customization may be out of scope for many smaller businesses, unless they are security conscious and technically capable. &amp;nbsp;It is, however,&amp;nbsp;a completely reasonable behavior for larger organizations.&amp;nbsp; So, why is it that most large corporations are not engaging in this security practice? After all, doing this measurably increases the difficulty and cost for an adversary to exploit the more uniform (hardened) environment.&amp;nbsp;&lt;/p&gt;&lt;p&gt;We believe that there are two major contributing factors that prevent organizations from engaging in the practice of customizing and hardening their builds of open source distributions. &amp;nbsp;First, of course, they might not know or think to do so. &amp;nbsp;After all, there is a lot of ignorance and learned helplessness out there, and consumers of all types and sizes have been conditioned to just accept the software they are given, and not think of tinkering or customizing it for their needs. &amp;nbsp;&lt;/p&gt;&lt;p&gt;Second, though, most corporations do not have any significant incentive to engage in this practice. &amp;nbsp;It might improve their security, but it is viewed as a cost in the short term to put in the effort. &amp;nbsp;If the weaker security stance results in a breach, well, that is why they have insurance... right? &amp;nbsp;&lt;/p&gt;&lt;p&gt;This wouldn't fly for most other types of risk management. &amp;nbsp;High value insurance policies can get pretty specific about what actions and levels of effort need to be taken by the insured to protect their property and prevent loss or damage. &amp;nbsp;The insurance company knows, for example, the TL-ratings for the safes used in high-value environments.&amp;nbsp;If the safe's rating says that it should last for about 30 minutes (TL-30) against common attacks, then the policy may require that a security guard pass by at least every 25 minutes. &amp;nbsp;&lt;/p&gt;&lt;p&gt;This sort of system incentivizes companies to protect themselves as much as reasonably possible. &amp;nbsp;It is one of the reasons why large organizations also have large physical security infrastructures that are customized for their business and environment. &amp;nbsp;A large multinational corporation doesn't just put a &quot;Secured by ___ Home Security&quot; sticker in their window and call it a day. &amp;nbsp;But, when it comes to cyber security practices, many corporations are frequently doing the equivalent, because cyber insurance policies are not requiring that they do more. &amp;nbsp;&amp;nbsp;&lt;/p&gt;&lt;p&gt;With these measurements and analytics, and the more detailed static, dynamic, and predictive ones to come, we hope to help address this issue.&lt;/p&gt;</content><author><name></name></author><summary type="html">A while back, we showed what the score distributions were for base installs of three major platforms.</summary></entry><entry><title type="html">Software Application Risks on the OSX Continuum</title><link href="https://cyber-itl.org/data/our_approach/2016/09/15/a-closer-look-at-the-osx-continuum.html" rel="alternate" type="text/html" title="Software Application Risks on the OSX Continuum" /><published>2016-09-15T00:00:00-07:00</published><updated>2016-09-15T00:00:00-07:00</updated><id>https://cyber-itl.org/data/our_approach/2016/09/15/a-closer-look-at-the-osx-continuum</id><content type="html" xml:base="https://cyber-itl.org/data/our_approach/2016/09/15/a-closer-look-at-the-osx-continuum.html">&lt;p&gt;In our &lt;a href=&quot;/2016/09/01/score-distributions-in-osx-win10-and-linux.html&quot;&gt;previous post&lt;/a&gt; about the score histograms for Windows, Linux, and OSX, we promised deeper dives to come.&lt;!--more--&gt; We also noted interesting things about each continuum and reminded people that the real value is being able to compare risk present in various software within a single continuum.&amp;nbsp; No we will take our first look at where some applications of interest live on the score continuum for OSX. &amp;nbsp;We'll look at three categories of software here: browsers, office suites, and update software. &amp;nbsp;All of these are targets for attack, so security is a concern for all of them. &amp;nbsp;The extent to which security influences purchasing decisions in these categories likely varies a great deal, however. &amp;nbsp;We'll discuss this aspect in more detail for each group as they're introduced. [Note: As stated before, these articles are to familiarize people with the value and limitation of static measures of risk. Once we finish these blog posts we will begin describing our dynamic and predictive analysis values and how they work with our static measurements.]&lt;/p&gt;&lt;p&gt;First up are browsers. &amp;nbsp;Browsers are particularly interesting because it's easy to change which one you're using, and easier for security to be a main consideration in the choice of which to use. &amp;nbsp;After all, they all provide you access to the same web content, they're a major vector for infection, and most of them are free, so cost isn't usually a deciding factor. &amp;nbsp;For corporations it is often most efficient to pick a single browser to configure and support. So if you are an OS X based shop, how would you chose the browser with the strongest security build and developer hygiene characteristics?&lt;/p&gt;&lt;p&gt;You might remember that the histogram of all OSX El Captain scores looked like so (as of August 2016):&amp;nbsp;&lt;/p&gt;

&lt;img src=&quot;/assets/images/blog/a-closer-look-at-the-osx-continuum-1.png&quot; alt=&quot;&quot; /&gt;


&lt;p&gt;The scores increase as you go from left to right, so applications lacking basic security artifacts will fall further to the left, and better built applications (from a security/risk perspective), will be further to the right. &amp;nbsp;To make this more concrete, let's look at where our three main OSX browsers fall on the continuum. &amp;nbsp;&lt;/p&gt;

&lt;img src=&quot;/assets/images/blog/a-closer-look-at-the-osx-continuum-1.png&quot; alt=&quot;&quot; /&gt;


&lt;p&gt;The line below the histogram is our relative hardening line. It is mapped to the histogram above, and used to notate where a particular piece of software (and it's supporting libraries) falls on the soft-to-hard scale. The yellow triangle on the histogram above the relative hardening line is aligned with its binary package below to help the viewer see where they land in the overall distribution.&lt;/p&gt;&lt;p&gt;With the browsers, the story is pretty clear - there are pretty big score differences between our three exemplars currently available on OS X, with Firefox (v 39.0) close to the 5th percentile mark, Safari (v 9.0)&amp;nbsp;in the middle (of the browsers, but below the 50th percentile mark for the OS X environment), and Chrome (v 48.0.2564)&amp;nbsp;leading the pack with a more respectable score. &amp;nbsp; We were surprised to see that Firefox had no ASLR enabled, so we did some digging and discovered that the feature had been &lt;a href=&quot;https://bugzilla.mozilla.org/show_bug.cgi?id=1290675#c0&quot;&gt;intentionally disabled by the development team&lt;/a&gt; so that it could be backwards compatible with OSX 10.6. &amp;nbsp;That backwards compatibility is planned on being dropped in the next major Firefox release, v 49.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Keep in mind that the scores for these browsers and other applications, are only meaningfully compared within the same operating environment.&amp;nbsp;For Windows 10 and Linux the same browsers we are showing here in OS X ranked and scored significantly differently. No wonder there has not been a clear winner to the browser security holy wars (yet);&amp;nbsp;depending on the browser &lt;em&gt;and &lt;/em&gt;the OS, the correct answer differs!&amp;nbsp;When we measure these features, it becomes readily apparent as to which browser is more difficult for an adversary to newly exploit. &amp;nbsp;Perhaps one of the browser developers will use the information we will be producing to become the clear leader across all available platforms.&lt;/p&gt;&lt;p&gt;This leads us to one of the questions we get asked a lot, which is how often will we reevaluate products? &amp;nbsp;Will we update our reports each time a patch is issued, for example? &amp;nbsp;We could do this, and in fact we have during our development process, but it is not our current plan.&amp;nbsp;Extracting the hundreds of build features, complexity measurements, and artifacts of developer hygiene for the static portion of our risk analysis only takes, on average, a few seconds per target and it is easily distributed and performed in parallel. However, it turns out that in practice we really do not need to re-evaluate for every minor revision,&amp;nbsp;because a single minor patch usually doesn't change enough to matter.&amp;nbsp; For organizations and consumers who provide funding for such services we could do this, and for the fine grained feature analysis that we described in the BlackHat and Defcon talks this can be meaningful,&amp;nbsp;but in general we plan on focusing our public reports on each major release of a product.&amp;nbsp;Major revisions have shown significant changes for better and worse in the products we have analyzed, but minor changes from patch to patch seldom move the needle much. &amp;nbsp;As an example,&amp;nbsp;looking at several versions of Chrome [48.0.2564 to 50.0.2657]&amp;nbsp;showed precisely the same overall score.&amp;nbsp; After all, a patch typically only touches a small fraction of an application's total lines of code. &amp;nbsp;The purpose of the patch is to fix a specific bug or vulnerability instance,&amp;nbsp;not address whole classes of vulnerabilities or change fundamental design elements (unfortunately). &amp;nbsp;&lt;/p&gt;&lt;p&gt;Our next category of reviewed software is office suites. &amp;nbsp;Office suites (typically word processing, spreadsheet, and presentation software), are a significant target in phishing and spearphishing attacks. If a user opens an attachment in an office suite application it is largely up to that application to prevent malicious activity and execution.&amp;nbsp;While traditionally there has always been one dominant suite that everyone uses by default, there are options available here. &amp;nbsp;On an environment like OS X there are even multiple commercial-quality options. &amp;nbsp;We do appreciate that people have a more difficult time switching between suites, though, because office suites vary more in their features and capabilities, so security usually is not a sole driving factor in decision-making. &amp;nbsp;The average consumer might not be aware, however, of the options that are now available to them, or how much the security might vary from one offering to another within OS X solutions. &amp;nbsp;In the following chart we overlay Microsoft Office 2011, iWork, and open source office offerings (LibreOffice 5.1.0 and Apache's OpenOffice 4.1.2)&amp;nbsp;on the existing continuum that we just populated with the browsers above.&lt;/p&gt;

&lt;img src=&quot;/assets/images/blog/a-closer-look-at-the-osx-continuum-1.png&quot; alt=&quot;&quot; /&gt;


&lt;p&gt;Each office suite's software is based on the average of the scores for their main applications - their text editor, spreadsheet tool, and presentation application. For each of these applications the score is made from the core application, all of the libraries it loads, the libraries they load, and so on, as evaluated for the features described in our prior post.&amp;nbsp;So, the average of Word, Excel, and Powerpoint in Microsoft's office suite, or Pages, Numbers, and Keynote for the Apple office suite. &amp;nbsp;&lt;/p&gt;&lt;p&gt;Take a moment to reconsider the &lt;a href=&quot;/2016/09/01/score-distributions-in-osx-win10-and-linux.html&quot;&gt;previous blog post&lt;/a&gt;&amp;nbsp;where we showed the histogram scores for Windows 10, OS X, and Linux.&amp;nbsp;The histogram of scores for the base Windows 10 install showed that Microsoft was putting in a fair amount of effort to consistently use application armoring and good developer hygiene. &amp;nbsp;It is interesting, and disconcerting, to see how different that picture looks when you see Microsoft's offerings as built for a competitors' environment. &amp;nbsp;Microsoft's Office for OSX suite is all 32 bit binaries with executable heaps and no source fortification. &amp;nbsp;The vast majority of OSX 32 bit binaries had non-executable heaps, which means that the build environment for this office suite is either very out of date and/or idiosyncratic, or that this particular safety feature was specifically disabled. &amp;nbsp;Either explanation doesn't reflect very well on Microsoft. &amp;nbsp;&lt;/p&gt;&lt;p&gt;The other office suites do better, all landing around where Safari was. &amp;nbsp;We looked at both OpenOffice and LibreOffice, but they had enough code, build, and hygiene features in common that they are essentially interchangeable from a perspective of static binary scores, so they only have one data point on our chart. &amp;nbsp;The native Apple office suite does the best, although not by a huge margin. &amp;nbsp;These tools at least applied the default application armoring features that most applications had, and were 64 bit binaries, which is why they end up in the middle of the overall distribution. &amp;nbsp;These are all relatively soft targets, though it still makes sense to choose the one that introduces more work to the adversary from a security and safety perspective. It is unsurprising that new zero days continue to target these applications in this environment.&lt;/p&gt;&lt;p&gt;Finally, we looked at Auto Update software. &amp;nbsp;This isn't a category of software that consumers specifically choose to run, but rather something that they get as a side effect of other purchasing decisions. &amp;nbsp;Still, Auto Updaters are a major security target, and their security contributes to the overall cost of ownership for the products they're a part of. &amp;nbsp;After all, they accept some form of input from remote sources and they are often in a position of privilege within the operating system for installs and upgrades. The following chart adds Microsoft and Apple's update software to the continuum. One of the results is... disturbing.&amp;nbsp;&lt;/p&gt;

&lt;img src=&quot;/assets/images/blog/a-closer-look-at-the-osx-continuum-1.png&quot; alt=&quot;&quot; /&gt;


&lt;p&gt;We always look across all of the binaries to identify the very best and very worst applications in any run, to see if anything of interest pops out. &amp;nbsp;We were dismayed to see that Microsoft AutoUpdate (the update software that ships with their office suite) had one of the worst 10 scores out of all 38k+ binaries we evaluated. &amp;nbsp;It's a 32 bit application with no ASLR, an executable heap, unfortified source, and no stack guards. &amp;nbsp;In the office suite scores from the prior histogram, we did not include this update software. If we change the score of the office suite to include all the software it comes with, this pulls the score down significantly. It should be pointed out that this updater is related to Microsoft's DRM (digital rights management - anti software piracy) component that is listening on the network on your OS X system if you have MS Office installed.&amp;nbsp;&lt;/p&gt;&lt;p&gt;In contrast, the native OSX Auto Update software does quite well, scoring slightly better than Google Chrome did. &amp;nbsp;Given what a target auto updaters are for attackers, this is more along the lines of what we would like to see for this sort of software. &amp;nbsp;&lt;/p&gt;&lt;p&gt;It is clear that there is a variety of products available to consumers, with a lot of variation in their security and design quality. &amp;nbsp;And, as we see with the Microsoft products for OSX, a vendor that does well in one environment might do quite poorly in another. &amp;nbsp;There is no guarantee of consistency of development practices across a large organization. &amp;nbsp;However, with information such as this we are hoping to arm and inform consumers to help them make better and safer choices for their particular environments.&lt;/p&gt;</content><author><name></name></author><category term="osx" /><category term="browsers" /><category term="office suites" /><category term="autoupdate" /><category term="microsoft" /><category term="firefox" /><category term="static analysis" /><summary type="html">In our previous post about the score histograms for Windows, Linux, and OSX, we promised deeper dives to come.</summary></entry><entry><title type="html">Score Distributions in OSX, Win10, and Linux</title><link href="https://cyber-itl.org/2016/09/01/score-distributions-in-osx-win10-and-linux.html" rel="alternate" type="text/html" title="Score Distributions in OSX, Win10, and Linux" /><published>2016-09-01T00:00:00-07:00</published><updated>2016-09-01T00:00:00-07:00</updated><id>https://cyber-itl.org/2016/09/01/score-distributions-in-osx-win10-and-linux</id><content type="html" xml:base="https://cyber-itl.org/2016/09/01/score-distributions-in-osx-win10-and-linux.html">&lt;p&gt;The data we're sharing first is the data from what we refer to as our &lt;a href=&quot;/2016/09/01/our-static-analysis-metrics-and-features.html&quot;&gt;static analysis&lt;/a&gt;. &amp;nbsp;(Fuzzing and dynamic analysis data will be described later).&lt;!--more--&gt; This is the part of our data that is most similar to nutritional facts of software, as it focuses on features and contents of the binary. &amp;nbsp;In particular, what application armoring features are present (and how well were they done), how complex is it, and what does the developer hygiene look like? &amp;nbsp;Each binary gets a &quot;local score&quot;, based on just that binary, and a &quot;library score&quot;, based on the local scores for all libraries it depends on (directly or indirectly, through other libraries). &amp;nbsp;These two values are combined to produce the total score. &amp;nbsp;The charts below show histograms of the total scores for each of the tested environments. &amp;nbsp;&lt;/p&gt;&lt;p&gt;We have static analysis data on the base installs on all 3 of our initial desktop environments: OSX (El Capitan 10.11), Windows 10, and Ubuntu Linux (16.04 LTS). &amp;nbsp;Since it had come to our attention as an interesting test case, these installs also include &lt;a href=&quot;https://www.continuum.io/&quot;&gt;Anaconda&lt;/a&gt;, the big data analytics package from Continuum. &amp;nbsp;It's important to note that scores don't compare directly between environments. &amp;nbsp;If the 50th percentile mark for Windows is higher than the one for Linux, for example, that doesn't necessarily mean anything. &amp;nbsp;Each environment has different sets of safety features available and different hazards for programmers to avoid, so the score values aren't apples to apples. This is important enough to bear repeating: consumers should compare the ratings we will be releasing of applications against each other *within*&amp;nbsp;a particular environment. What we're focusing on here is the overall distribution and range of scores, as this tells us something about how consistent the development process was for that operating system/environment. &amp;nbsp;So which company appears to have the better Security Development Life Cycle (SDLC)&amp;nbsp;process for their base OS?&lt;/p&gt;&lt;p&gt;We're still finalizing some data before we share reports or scores for specific software verticals, but we can share the overall landscape in each environment. &amp;nbsp;In a near-future blog post we'll add call-outs for specific applications to these charts, allowing comparisons between competing projects. &amp;nbsp;For now, we're just getting the lay of the land. &amp;nbsp;First off, here's the histogram of application scores for the base installation of OSX. &amp;nbsp;&lt;/p&gt;
  
&lt;figure&gt;&lt;img src=&quot;/assets/images/blog/score-distributions-in-osx-win10-and-linux-1.png&quot; alt=&quot; Histogram of static analysis scores for base install of OSX El Capitan. &amp;nbsp; &quot; /&gt;&lt;figcaption&gt;Histogram of static analysis scores for base install of OSX El Capitan.&lt;/figcaption&gt;&lt;/figure&gt;
  

&lt;p&gt;Base OSX has a roughly bimodal distribution, with the scores covering a pretty broad range. &amp;nbsp;The 5th and 95th percentile lines are 67 points away from each other. &amp;nbsp;&amp;nbsp; While there aren't any perfect scores, there's a fair amount in the 90s (100 would be a &quot;perfect score&quot; for this high level static view). [Note: &quot;Perfect score&quot; does not mean secure. Instead, it means that the appropriate safety measures and protective harnesses were enabled in the development process. As an analogy, a car that has seat belts, anti-lock breaks, air bags, active and passive restraints, etc. would receive a &quot;perfect score&quot;. We could also call this the &quot;bare minimum&quot;, but we're trying to encourage the industry rather than simply berate it. In the future we will release the measurements that show the level of efficacy each of these safety features achieved per application, but for now we are discussing this at a much higher level to help acclimate people to the approach.]&amp;nbsp; &amp;nbsp;&lt;/p&gt;
  
&lt;figure&gt;&lt;img src=&quot;/assets/images/blog/score-distributions-in-osx-win10-and-linux-2.png&quot; alt=&quot; Similar histogram showing total score distribution in Ubuntu Linux. &amp;nbsp; &quot; /&gt;&lt;figcaption&gt;Similar histogram showing total score distribution in Ubuntu Linux.&lt;/figcaption&gt;&lt;/figure&gt;
  

&lt;p&gt;With Linux Ubuntu (16.04 LTS), we still have a pretty broad range of scores, but with a longer tail for the upper range of the scores. &amp;nbsp;Here we have more of a normal distribution. &amp;nbsp;The 95th percentile line moves down a bit, but the other percentile marks are surprisingly close to the values in OSX. &amp;nbsp;&lt;/p&gt;&lt;p&gt;With Windows, we have a very different picture. Frankly, this is very impressive:&amp;nbsp;&lt;/p&gt;
  
&lt;figure&gt;&lt;img src=&quot;/assets/images/blog/score-distributions-in-osx-win10-and-linux-3.png&quot; alt=&quot; Windows 10 score distribution. &amp;nbsp;All scores lower than -22 are from Anaconda. &amp;nbsp;Without Anaconda included the 5th percentile is a score of 53 (instead of 40). &amp;nbsp; &quot; /&gt;  &lt;figcaption&gt;Windows 10 score distribution. &amp;nbsp;All scores lower than -22 are from Anaconda. &amp;nbsp;Without Anaconda included the 5th percentile is a score of 53 (instead of 40). &lt;/figcaption&gt;&lt;/figure&gt; 
  

&lt;p&gt;Windows shows a pretty different chart from the previous two. &amp;nbsp;The bin for scores from 65-70 has about 5500 files in it. &amp;nbsp;Even accounting for the increased number of files in the Windows base install, this is much higher than the biggest peaks on either of the other charts, indicating that application of armoring and security features is much more uniform for the Windows environment. &amp;nbsp;Windows also has many more files with a perfect or near-perfect score than OSX or Linux had. &amp;nbsp;While all three environments had the potential for a final score to be slightly over 100, this is the only environment to have any in practice. &amp;nbsp;&lt;/p&gt;&lt;p&gt;This distribution will become a lot broader once we bring in third party applications. &amp;nbsp;This prediction is supported by what Anaconda's presence already does to the chart. &amp;nbsp;Namely, it contributes the lowest 450 or so scores to the chart, including everything scoring a -22 or lower. &amp;nbsp;[Note: Continuum, the company that packages Anaconda, is aware of these findings. Should they decide to move to a modern development with a full complement of safety features available (and frequently enabled by default), their scores will improve significantly. &amp;nbsp;We are hoping they do so, as their current product imposes significant risk on their customers, and it would be a great initial win if this got fixed. We will keep you posted on this topic.]&lt;/p&gt;&lt;p&gt;We are installing a wide variety of third party software onto our test systems. This will provide an understanding of the overall software ecosystem for each environment. &amp;nbsp;More importantly, though, it'll allow consumers to call out all members of a particular software category and compare them based on where they fall on this safety continuum. &amp;nbsp;&lt;/p&gt;&lt;p&gt;With this information it will become a straight forward process to choose and use software that increases the cost to the adversary, and thus measurably decreases the risk to yourself.&amp;nbsp;After all, who would want to install and run the easiest target software for no reason?&lt;/p&gt;</content><author><name></name></author><summary type="html">The data we're sharing first is the data from what we refer to as our static analysis. &amp;nbsp;(Fuzzing and dynamic analysis data will be described later).</summary></entry><entry><title type="html">Our Static Analysis Metrics and Features</title><link href="https://cyber-itl.org/2016/09/01/our-static-analysis-metrics-and-features.html" rel="alternate" type="text/html" title="Our Static Analysis Metrics and Features" /><published>2016-09-01T00:00:00-07:00</published><updated>2016-09-01T00:00:00-07:00</updated><id>https://cyber-itl.org/2016/09/01/our-static-analysis-metrics-and-features</id><content type="html" xml:base="https://cyber-itl.org/2016/09/01/our-static-analysis-metrics-and-features.html">&lt;p&gt;If you've seen &lt;a href=&quot;/2016/09/01/score-distributions-in-osx-win10-and-linux.html&quot;&gt;our post&lt;/a&gt; about the score distributions in OSX, Linux, and Windows 10 base installs, your first question is probably about what factors go into computing those scores.&lt;!--more--&gt; &amp;nbsp;This post will provide some high level understanding of what factors we consider for those static analysis scores. &amp;nbsp;&lt;/p&gt;&lt;p&gt;The main question we're trying to find the answer to is, &quot;How difficult is it for an attacker to find a new exploit for this software?&quot;. &amp;nbsp;Attackers have limited resources, and just like anyone else, they don't like to waste their time doing something the hard way if an easier path is available. &amp;nbsp;They have tricks and heuristics they use to assess the relative difficulty to exploit software, so that they can focus on the easy targets and low hanging fruit. &amp;nbsp;Mudge has had a long career of doing just that (legally, and for research purposes), so he's developed his own personal toolkit of measurements to take when assessing software risk. &amp;nbsp;By consulting with other luminaries of the security field who have extensive exploit development experience, we've been able to build up the list of static analysis metrics and features which we currently assess.&lt;/p&gt;&lt;p&gt;There are three main categories of static analysis features that we look at for a software binary:&amp;nbsp;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Complexity&amp;nbsp;&lt;/li&gt;&lt;li&gt;Application Armoring&lt;/li&gt;&lt;li&gt;Developer Hygiene&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;For each of these, I'm going to list some of the main features and why these are important to have when estimating software risk. &amp;nbsp;&lt;/p&gt;&lt;h2&gt;Complexity&lt;/h2&gt;&lt;p&gt;First up, why does complexity matter? &amp;nbsp;Because more complex code is harder to review and maintain, and is more likely to contain bugs. &amp;nbsp;This is why &lt;a href=&quot;http://lars-lab.jpl.nasa.gov/JPL_Coding_Standard_C.pdf&quot;&gt;NASA/JPL&lt;/a&gt; put limits on things like function size for code that's going into critical systems. &amp;nbsp;&lt;/p&gt;&lt;p&gt;We look at more complex measurements like cyclomatic complexity, but my favorite features are the simpler ones:&amp;nbsp;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Code Size&lt;/li&gt;&lt;li&gt;Number of Conditional Branches&lt;/li&gt;&lt;li&gt;Size &amp;amp; Number of Stack Adjusts&lt;/li&gt;&lt;li&gt;Function Size&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Measurements like code size, number of branches and stack adjusts have a roughly lognormal distribution. &amp;nbsp;In general, we expect the density of branches to be constant. &amp;nbsp;That is, as code size increases, the counts for branches or stack adjusts should increase correspondingly. &amp;nbsp;If the number of branches is highly disproportionate compared to the code size, giving the code a high branch density,&amp;nbsp;that indicates a high level of complexity (and thus a higher level of risk). &amp;nbsp;&lt;/p&gt;&lt;h2&gt;Application Armoring&lt;/h2&gt;&lt;p&gt;Modern compilers, linkers, and loaders come with lots of nifty safety features, but they won't do you any good if the software doesn't have them enabled. &amp;nbsp;These features are to software what airbags and seatbelts are to cars: things that are known and proven to improve safety, and whose use should be established by now as industry-standard. &amp;nbsp;If your car doesn't have airbags, you're entitled to know that before you buy it. &amp;nbsp;&lt;/p&gt;&lt;p&gt;We've broken these features out by which part of the software lifecycle is responsible for implementing it.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Compiler&lt;ul style=&quot;margin-left:40px&quot;&gt;&lt;li&gt;Stack Guards&lt;/li&gt;&lt;li&gt;Function Fortification&lt;/li&gt;&lt;li&gt;Control Flow Integrity (CFI/CPI)&lt;/li&gt;&lt;li&gt;...&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;Linker&lt;ul style=&quot;margin-left:40px&quot;&gt;&lt;li&gt;Address Space Layout Randomiziation (ASLR)&lt;/li&gt;&lt;li&gt;Segment and Section ordering&lt;/li&gt;&lt;li&gt;...&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;Loader&lt;ul style=&quot;margin-left:40px&quot;&gt;&lt;li&gt;Section, Segment execution chars&lt;/li&gt;&lt;li&gt;Allocations and access&lt;/li&gt;&lt;li&gt;Code signing and/or verification&lt;/li&gt;&lt;li&gt;...&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;Developer Hygiene&lt;/h2&gt;&lt;p&gt;There are things we can learn about a developer's security skill and knowledge based on what functions are used in the code they write. &amp;nbsp;Some functions, like strcpy and strcat, are hard to use without introducing vulnerabilities. &amp;nbsp;Less risky functions like strncpy and strncat are better, but still easy to use incorrectly. &amp;nbsp;On the other hand, there are functions like strlcpy and strlcat, which were written with security considerations in mind, and which are difficult to use &lt;em&gt;incorrectly&lt;/em&gt;. &amp;nbsp;Finally, there are a few functions, such as system or gets, that should never be used in commercial code, because they simply pose too great of a security risk. &amp;nbsp;&lt;/p&gt;&lt;p&gt;We call these four categories of fuctions &quot;bad&quot;, &quot;risky&quot;, &quot;good&quot;, and &quot;ick&quot;. &amp;nbsp;We have about 500 POSIX and ANSI functions that fall into these categories, and by looking at the frequency, count, and consistency of the function categories used we can learn a lot about the developer practices of a particular software vendor. &amp;nbsp;&lt;/p&gt;&lt;p&gt; &lt;/p&gt;</content><author><name></name></author><summary type="html">If you've seen our post about the score distributions in OSX, Linux, and Windows 10 base installs, your first question is probably about what factors go into computing those scores.</summary></entry><entry><title type="html">Other Industries that Inspired Us</title><link href="https://cyber-itl.org/our_approach/2016/08/15/other-industries-that-inspired-us.html" rel="alternate" type="text/html" title="Other Industries that Inspired Us" /><published>2016-08-15T00:00:00-07:00</published><updated>2016-08-15T00:00:00-07:00</updated><id>https://cyber-itl.org/our_approach/2016/08/15/other-industries-that-inspired-us</id><content type="html" xml:base="https://cyber-itl.org/our_approach/2016/08/15/other-industries-that-inspired-us.html">&lt;p&gt;Evaluating the risk profile of software is a technically complex task, but there are lots of other industries where consumers have to engage in complex decision-making. &lt;!--more--&gt;&amp;nbsp;Choosing which car is the best choice for you, or which food best fits your diet, or which new refrigerator to buy are all very technically complex decisions, but those industries have all developed (mandatory) labels to help consumers stay informed. &amp;nbsp;We're drawing upon these labels as inspiration for our own reports. &amp;nbsp;&lt;/p&gt;
  
&lt;figure&gt;&lt;img src=&quot;/assets/images/blog/other-industries-that-inspired-us-1.png&quot; alt=&quot; Nutritional facts label from nutritiondata.com. &quot; class=&quot;small-image&quot; /&gt;  &lt;figcaption&gt;Nutritional facts label from nutritiondata.com.&lt;/figcaption&gt; &lt;/figure&gt;
  

&lt;p&gt;We're going to have future posts that go into more detail on the types of data we collect about software, but here we'll look at broad categories and how they're similar to data captured in the labels from other industries. &amp;nbsp;&lt;/p&gt;&lt;p&gt;We've already had &lt;a href=&quot;/2016/07/14/problem-with-standards-and-certifications.html&quot;&gt;a post&lt;/a&gt; where we compared our approach to nutritional facts labels. &amp;nbsp;The static analysis portion of our data, in particular, can be thought of as the &quot;nutritional facts of software&quot;. &amp;nbsp;It's where we talk about what went into making the software, good and bad - what safety features are present, and what things do we see that usually correlate with vulnerable code or risky programming practices. &amp;nbsp;These are things we can determine from inspecting the binary, without running it. &amp;nbsp;&lt;/p&gt;
  
                                                                                                                                                                                                                                             &lt;figure&gt;&lt;img src=&quot;/assets/images/blog/other-industries-that-inspired-us-2.png&quot; alt=&quot; That blue sticker you always see in the windows of new cars in the showroom. &quot; class=&quot;small-image&quot; /&gt; &lt;figcaption&gt;That blue sticker you always see in the windows of new cars in the showroom.&lt;/figcaption&gt;&lt;/figure&gt;
  

&lt;p&gt;We get our dynamic analysis data, on the other hand, from fuzzing and crash testing the software. &amp;nbsp;This is similar to the crash test, mileage per gallon, and environmental performance data that you find on the Monroney sticker. &amp;nbsp;While you might not be familiar with that name, if you've ever shopped for a new car in the US then you've seen this sticker displayed on all the cars in the dealership showroom. &amp;nbsp;These are all there to provide the buyer with information about the safety and cost of ownership of their prospective new car. &amp;nbsp;Interestingly, some of this data is based on mathematical modeling. &amp;nbsp;&lt;/p&gt;&lt;p&gt;While the crash test data is always based on actually crashing the car, the MPG data is frequently based on what the EPA &lt;em&gt;predicts&lt;/em&gt;&amp;nbsp;the MPG will be. &amp;nbsp;They've physically tested enough cars to be confident in predicting how other cars will do, based on other facts about that model. &amp;nbsp;&lt;/p&gt;&lt;p&gt;This is our model for dynamic analysis as well - test enough software samples with actual fuzzing to be able to model and predict with high accuracy what categories and types of crashes we'd see for other software. &amp;nbsp;&lt;/p&gt;
  &lt;figure&gt;
      &lt;img src=&quot;/assets/images/blog/other-industries-that-inspired-us-3.png&quot; alt=&quot;&quot; class=&quot;small-image&quot;/&gt;
      &lt;/figure&gt;
  

&lt;p&gt;Finally, the Energyguide label is required to be displayed on several types of new appliances. &amp;nbsp;It tells you the operating cost for the device, and puts it on a continuum relative to competing products. &amp;nbsp;Similarly, we'll place software scores onto a safety continuum. &amp;nbsp;This continuum will let you know how a given piece of software did compared to competing products, and compared to overall software in its environment. &amp;nbsp;For example, we'd tell you how Internet Explorer did compared to other Windows browsers such as Edge, Chrome, and Firefox, but also where it falls in the overall spectrum of Windows software. &amp;nbsp;&lt;/p&gt;&lt;p&gt;In the next posts, we'll be going into more detail on these different categories of data, describing what measurements fall into each, and sharing some examples of preliminary data from those groups. &amp;nbsp;All of these categories will play a role in our final, public-facing reports. &amp;nbsp;&lt;/p&gt;
  
      &lt;img src=&quot;/assets/images/blog/other-industries-that-inspired-us-4.png&quot; alt=&quot;&quot; class=&quot;small-image&quot;/&gt;</content><author><name></name></author><category term="reports" /><category term="labels" /><category term="energyguide" /><category term="monroney" /><category term="nutrition" /><summary type="html">Evaluating the risk profile of software is a technically complex task, but there are lots of other industries where consumers have to engage in complex decision-making.</summary></entry></feed>